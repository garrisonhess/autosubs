#### Configuration
DEBUG: False
VAL: False
TOY_DATA: False
threshold: 256
amp: False     # AMP helps w/ memory but causes gradient explosions
experiment_name: exp
num_workers: 6
pin_memory: True
train_shuffle: True
val_shuffle: False
device: "cuda:0"
run_inference: False
input_dim: 40
update_freq: 24
channels_last: False
val_workers: 4
val_batch_size: 128
min_tf: 0.5
grad_clip: 2.0

# ray
experiment_name: exp
verbosity: 3

# ASHA
epochs: 200
grace_period: 200
reduction_factor: 2
brackets: 1
num_samples: 1

# architectural parameters
enc_h: [256]
dec_h: [512]
embed_dim: [256]
attn_dim: [128]
conv_channels: [0]
enc_dropout: [0.3]
dec_dropout: [0.3]
use_multihead: [False]
nheads: [1]
lock_drop: [False]
spec_augment: True
proj_size: 0
enc_ln: False
dec_ln: False
weight_tying: True
encoder_arch: 
        - [lstm, lstm, plstm, plstm, plstm, lstm]
minival: True

# hyperparameters
lr: [0.001]
weight_decay: [0.000005]
gamma: [0.5]
batch_size: [96]
lr_step: [30]
smooth_temp: 0.0


# transfer learning configuration
warmup_epochs: 0
pretrained_decoder: True
pretrained_full: False #True
warmup_zero_context: True
decoder_ckpt_path: ~/hw4p2/decoder_ckpts/decoder-2021-04-28-23-0617-epoch20-dist376-inner_0f2ed_00000.pth
full_ckpt_path: ~/hw4p2/full_ckpts/full-2021-04-29-11-5452-epoch33-dist105-inner_4eeab_00000.pth


# schedules
tf_init: 0.90
tf_drop_every: 25
tf_drop: 0.1


# PATHS
train_path: ~/hw4p2/data/train.npy
train_transcripts_path: ~/hw4p2/data/train_transcripts.npy
val_path: ~/hw4p2/data/dev.npy
val_transcripts_path: ~/hw4p2/data/dev_transcripts.npy
test_path: ~/hw4p2/data/test.npy
results_path: ~/hw4p2/results/
ray_results_dir: ~/hw4p2/ray_logs/
checkpoints_path: ~/hw4p2/checkpoints/
decoder_ckpt_dir: ~/hw4p2/decoder_ckpts/
debug_train_path: ~/hw4p2/data/debug/train.npy
debug_train_transcripts_path: ~/hw4p2/data/debug/train_transcripts.npy
debug_val_path: ~/hw4p2/data/debug/dev.npy
debug_val_transcripts_path: ~/hw4p2/data/debug/dev_transcripts.npy
plot_path: ~/hw4p2/plots/
